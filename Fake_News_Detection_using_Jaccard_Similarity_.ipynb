{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09e76a84",
      "metadata": {
        "id": "09e76a84"
      },
      "outputs": [],
      "source": [
        "# DO NOT RUN THIS AGAIN\n",
        "#!pip install -U textblob\n",
        "#!python -m textblob.download_corpora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7c4c13c",
      "metadata": {
        "id": "c7c4c13c"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn import tree\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from textblob import Blobber\n",
        "from textblob import TextBlob\n",
        "from textblob.taggers import NLTKTagger\n",
        "from textblob.wordnet import Synset\n",
        "from textblob import Word\n",
        "from textblob.wordnet import NOUN\n",
        "from textblob.wordnet import VERB\n",
        "#from textblob.wordnet import ADJ\n",
        "#from textblob.wordnet import ADVERB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf3058dd",
      "metadata": {
        "id": "bf3058dd"
      },
      "outputs": [],
      "source": [
        "# Load the data into a pandas dataframe and split it up\n",
        "df = pd.read_csv(\"train.csv\")\n",
        "\n",
        "df_train = df.sample(frac=0.8)\n",
        "df_test = df.drop(df_train.index)\n",
        "\n",
        "y_train = df_train.iloc[:,[-1]]\n",
        "X_train = df_train.drop(y_train.columns, axis=1)\n",
        "\n",
        "y_test = df_test.iloc[:,[-1]]\n",
        "X_test = df_test.drop(y_test.columns, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b204c8c5",
      "metadata": {
        "id": "b204c8c5"
      },
      "source": [
        "## Steps\n",
        "1. Take fake news articles and create TF-IDF (might want to cut duplicate titles)\n",
        "2. Strip out articles & prepositions\n",
        "3. See what words are most \"fake newsy\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "10f92851",
      "metadata": {
        "id": "10f92851"
      },
      "outputs": [],
      "source": [
        "# # Drop duplicates to prevent headlines that appear a lot from skewing the data\n",
        "# fake_news = X_train[\"title1_en\"].drop_duplicates().tolist()\n",
        "\n",
        "# # Gross, disgusting regex to cut stop words and other silliness\n",
        "# for i in range(len(fake_news)):\n",
        "#     fake_news[i] = re.sub('(\\s+)(a|an|and|the|of|from|to|by|in|is|#|aaa+(\\.*)*|<\\s+i\\s+>)(\\s+)', ' ', fake_news[i])\n",
        "#     fake_news[i] = re.sub('A(\\s+)', '', fake_news[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d399c19c",
      "metadata": {
        "id": "d399c19c"
      },
      "outputs": [],
      "source": [
        "# # Cram this stuff into a vectorizer\n",
        "# vectorizer = TfidfVectorizer()\n",
        "# fake_news_tfidf = vectorizer.fit_transform(fake_news)\n",
        "# words = vectorizer.get_feature_names()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9266b62f",
      "metadata": {
        "id": "9266b62f"
      },
      "outputs": [],
      "source": [
        "# # We only need the first document, turn it into a dataframe so we can have a look\n",
        "# first_doc_vector = fake_news_tfidf[0]\n",
        "# df_fake_news_tfidf = pd.DataFrame(data=first_doc_vector.T.todense(), index=words, columns=[\"score\"])\n",
        "# df_fake_news_tfidf.sort_values(by=\"score\", ascending=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a59d250",
      "metadata": {
        "id": "8a59d250"
      },
      "source": [
        "## Now we do something with this: Jaccard Similarity\n",
        "\n",
        "Now that we know what looks kinda like fake news, try and see which articles are related."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dee3e4a",
      "metadata": {
        "id": "5dee3e4a"
      },
      "outputs": [],
      "source": [
        "def jaccard(string1, string2):\n",
        "    string1 = re.sub('(\\s+)(a|an|and|the|of|from|to|by|in|is|#|aaa+(\\.*)*|<\\s+i\\s+>)(\\s+)', ' ', string1)\n",
        "    string1 = re.sub('A(\\s+)|The(\\s+)', '', string1)\n",
        "    string1 = re.sub(r'[^\\w\\s]', '', string1)\n",
        "\n",
        "    string2 = re.sub('(\\s+)(a|an|and|the|of|from|to|by|in|is|#|aaa+(\\.*)*|<\\s+i\\s+>)(\\s+)', ' ', string2)\n",
        "    string2 = re.sub('A(\\s+)', '', string2)\n",
        "    string2 = re.sub(r'[^\\w\\s]', '', string2)\n",
        "    \n",
        "    list1 = string1.split()\n",
        "    set1 = set(list1)\n",
        "    \n",
        "    list2 = string2.split()\n",
        "    set2 = set(list2)\n",
        "    \n",
        "    numerator = float(len(set1.intersection(set2)))\n",
        "    denominator = len(set1.union(set2))\n",
        "    \n",
        "    return numerator/denominator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbfa1687",
      "metadata": {
        "id": "dbfa1687"
      },
      "outputs": [],
      "source": [
        "def jaccard_sim(list1, list2):\n",
        "    set1 = set(list1)\n",
        "    \n",
        "    set2 = set(list2)\n",
        "    \n",
        "    numerator = float(len(set1.intersection(set2)))\n",
        "    denominator = len(set1.union(set2))\n",
        "    \n",
        "    if denominator == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return numerator/denominator"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f722f5c",
      "metadata": {
        "id": "0f722f5c"
      },
      "source": [
        "Extract Jaccard score as a feature."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9352f88d",
      "metadata": {
        "id": "9352f88d"
      },
      "outputs": [],
      "source": [
        "# total = []\n",
        "\n",
        "# for index, x in X_train.iterrows():\n",
        "#     total.append(jaccard(x[\"title1_en\"], x[\"title2_en\"]))\n",
        "\n",
        "# X_train[\"jaccard\"] = total\n",
        "\n",
        "# X_objective = X_train[[\"id\", \"jaccard\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b9903c6",
      "metadata": {
        "id": "8b9903c6"
      },
      "outputs": [],
      "source": [
        "# clf = tree.DecisionTreeClassifier(max_depth=1)\n",
        "# clf.fit(X_objective, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56e9272",
      "metadata": {
        "id": "f56e9272"
      },
      "outputs": [],
      "source": [
        "# y_train.value_counts() # So we can label things correctly"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "00ce38b4",
      "metadata": {
        "scrolled": true,
        "id": "00ce38b4"
      },
      "outputs": [],
      "source": [
        "# listo = []\n",
        "\n",
        "# for index, x in X_test.iterrows():\n",
        "#     listo.append(jaccard(x[\"title1_en\"], x[\"title2_en\"]))\n",
        "    \n",
        "# X_test[\"jaccard\"] = listo\n",
        "\n",
        "# X_test_o = X_test[[\"id\", \"jaccard\"]]\n",
        "\n",
        "# y_pred = clf.predict(X_test_o)\n",
        "\n",
        "# print(metrics.classification_report(y_test, y_pred, target_names=[\"agreed\", \"disagreed\", \"unrelated\"]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b404a54c",
      "metadata": {
        "id": "b404a54c"
      },
      "outputs": [],
      "source": [
        "#tree.plot_tree(clf)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4950e683",
      "metadata": {
        "id": "4950e683"
      },
      "source": [
        "## On to the next thing\n",
        "\n",
        "We achieve 75% accuracy by checking title similarities, but we can go even further beyond. This will require more in-depth analysis. Notably, we don't detect *any* disagreed articles and our detection of agreed articles is still not quite there yet."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "72a072f7",
      "metadata": {
        "id": "72a072f7"
      },
      "source": [
        "## Let's diagram the sentences and give them a score\n",
        "\n",
        "The idea is as follows:\n",
        "1. Tokenize & diagram headline 1 and headline 2\n",
        "2. Give each a score based on noun, verb, adj/adv similarity (normalized by length of sentence)\n",
        "3. See where this takes us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7bff155",
      "metadata": {
        "id": "e7bff155"
      },
      "outputs": [],
      "source": [
        "def score_pos_sims(string1, string2):\n",
        "    tagger = Blobber(pos_tagger=NLTKTagger())\n",
        "\n",
        "    pnoun_list = [\"NNP\", \"NNPS\"]\n",
        "    noun_list = [\"NN\", \"NNS\", \"PRP\", \"PRP$\"]\n",
        "    verb_list = [\"VB\", \"VBP\", \"VBZ\", \"VBD\", \"VBN\", \"VBG\"]\n",
        "    adj_list = [\"CD\", \"JJS\", \"JJR\"]\n",
        "    adv_list = [\"RBS\", \"RBR\", \"RB\"]\n",
        "\n",
        "    nouns1 = []\n",
        "    verbs1 = []\n",
        "    pnouns1 = []\n",
        "    adjs1 = []\n",
        "    advs1 = []\n",
        "    \n",
        "    nouns2 = []\n",
        "    verbs2 = []\n",
        "    pnouns2 = []\n",
        "    adjs2 = []\n",
        "    advs2 = []\n",
        "\n",
        "    blob1 = tagger(string1)\n",
        "    blob2 = tagger(string2)\n",
        "\n",
        "    for x in blob1.tags:\n",
        "        if x[1] in noun_list:\n",
        "            w = Word(x[0])\n",
        "            w = w.lemmatize(\"n\")\n",
        "            nouns1.append(w.lower())\n",
        "        elif x[1] in pnoun_list:\n",
        "            pnouns1.append(x[0].lower())\n",
        "        elif x[1] in verb_list:\n",
        "            w = Word(x[0])\n",
        "            w = w.lemmatize(\"v\")\n",
        "            verbs1.append(x[0].lower())\n",
        "        elif x[1] in adj_list:\n",
        "            adjs1.append(x[0].lower())\n",
        "        elif x[1] in adv_list:\n",
        "            advs1.append(x[0].lower())\n",
        "\n",
        "    for x in blob2.tags:\n",
        "        if x[1] in noun_list:\n",
        "            w = Word(x[0])\n",
        "            w = w.lemmatize(\"n\")\n",
        "            nouns2.append(w.lower())\n",
        "        elif x[1] in pnoun_list:\n",
        "            pnouns2.append(x[0].lower())\n",
        "        elif x[1] in verb_list:\n",
        "            w = Word(x[0])\n",
        "            w = w.lemmatize(\"v\")\n",
        "            verbs2.append(x[0].lower())\n",
        "        elif x[1] in adj_list:\n",
        "            adjs2.append(x[0].lower())\n",
        "        elif x[1] in adv_list:\n",
        "            advs2.append(x[0].lower())\n",
        "\n",
        "    result = [None] * 5\n",
        "    result[0] = jaccard_sim(pnouns1, pnouns2)\n",
        "    result[1] = jaccard_sim(nouns1, nouns2)\n",
        "    result[2] = jaccard_sim(verbs1, verbs2)\n",
        "    result[3] = jaccard_sim(adjs1, adjs2)\n",
        "    result[4] = jaccard_sim(advs1, advs2)\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d6cbea",
      "metadata": {
        "id": "42d6cbea"
      },
      "outputs": [],
      "source": [
        "pnoun_sim = []\n",
        "noun_sim = []\n",
        "verb_sim = []\n",
        "adj_sim = []\n",
        "adv_sim = []\n",
        "tot_sim = []\n",
        "\n",
        "for index, x in X_train.iterrows():\n",
        "    string1 = x[\"title1_en\"]\n",
        "    string2 = x[\"title2_en\"]\n",
        "    \n",
        "    result = score_pos_sims(string1, string2)\n",
        "    \n",
        "    pnoun_sim.append(result[0])\n",
        "    noun_sim.append(result[1])\n",
        "    verb_sim.append(result[2])\n",
        "    adj_sim.append(result[3])\n",
        "    adv_sim.append(result[4])\n",
        "    \n",
        "    tot_sim.append(jaccard(string1, string2))\n",
        "\n",
        "X_train[\"pnoun_sim\"] = pnoun_sim\n",
        "X_train[\"noun_sim\"] = noun_sim\n",
        "X_train[\"verb_sim\"] = verb_sim\n",
        "X_train[\"adj_sim\"] = adj_sim\n",
        "X_train[\"adv_sim\"] = adv_sim\n",
        "X_train[\"tot_sim\"] = tot_sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc5a9070",
      "metadata": {
        "id": "cc5a9070"
      },
      "outputs": [],
      "source": [
        "# Cram this into a Naive Bayes\n",
        "#mnb = MultinomialNB()\n",
        "\n",
        "# Or a Decision Tree, whichever does better\n",
        "#clf = tree.DecisionTreeClassifier(max_depth=5)\n",
        "\n",
        "X_train_objs = X_train.copy()\n",
        "X_train_objs = X_train_objs.drop([\"id\", \"tid1\", \"tid2\", \"title1_en\", \"title2_en\"], axis=1)\n",
        "\n",
        "#clf.fit(X_train_objs, y_train)\n",
        "\n",
        "#mnb.fit(X_train_objs, y_train[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2743eb41",
      "metadata": {
        "id": "2743eb41"
      },
      "outputs": [],
      "source": [
        "pnoun_sim = []\n",
        "noun_sim = []\n",
        "verb_sim = []\n",
        "adj_sim = []\n",
        "adv_sim = []\n",
        "tot_sim = []\n",
        "\n",
        "for index, x in X_test.iterrows():\n",
        "    string1 = x[\"title1_en\"]\n",
        "    string2 = x[\"title2_en\"]\n",
        "    \n",
        "    result = score_pos_sims(string1, string2)\n",
        "    \n",
        "    pnoun_sim.append(result[0])\n",
        "    noun_sim.append(result[1])\n",
        "    verb_sim.append(result[2])\n",
        "    adj_sim.append(result[3])\n",
        "    adv_sim.append(result[4])\n",
        "    \n",
        "    tot_sim.append(jaccard(string1, string2))\n",
        "\n",
        "X_test[\"pnoun_sim\"] = pnoun_sim\n",
        "X_test[\"noun_sim\"] = noun_sim\n",
        "X_test[\"verb_sim\"] = verb_sim\n",
        "X_test[\"adj_sim\"] = adj_sim\n",
        "X_test[\"adv_sim\"] = adv_sim\n",
        "X_test[\"tot_sim\"] = tot_sim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68a878eb",
      "metadata": {
        "id": "68a878eb"
      },
      "outputs": [],
      "source": [
        "X_test_o = X_test.copy()\n",
        "X_test_o = X_test_o.drop([\"id\", \"tid1\", \"tid2\", \"title1_en\", \"title2_en\"], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84aaf8de",
      "metadata": {
        "id": "84aaf8de",
        "outputId": "9736b776-eb21-411c-a44f-5e49a02ce908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-1e34e902ef69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOneVsRestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_objs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_o\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"agreed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"disagreed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"unrelated\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'OneVsRestClassifier' is not defined"
          ]
        }
      ],
      "source": [
        "y_pred = OneVsRestClassifier(RandomForestClassifier(max_depth=5, random_state=0)).fit(X_train_objs, y_train).predict(X_test_o)\n",
        "\n",
        "print(metrics.classification_report(y_test, y_pred, target_names=[\"agreed\", \"disagreed\", \"unrelated\"]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}